%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Duc Tien Dang Nguyen at 2019-02-18 21:14:39 +0100 


%% Saved with string encoding Unicode (UTF-8) 

@article{boato2020morphological,
	title={Morphological filter detector for image forensics applications},
	author={Boato, Giulia and Dang-Nguyen, Duc-Tien and De Natale, Francesco GB},
	journal={IEEE Access},
	volume={8},
	pages={13549--13560},
	year={2020},
	publisher={IEEE}
}

@article{gurrin2019invited,
	title={Comparing Approaches to Interactive Lifelog Search at the Lifelog Search Challenge (LSC2018)},
	author={Gurrin, Cathal and Schoeffmann, Klaus and Joho, Hideo and Leibetseder, Andreas and Zhou, Liting and Duane, Aaron and Dang-Nguyen, Duc-Tien and Riegler, Michael and Piras, Luca and Tran, Minh-Triet and others},
	journal={ITE Transactions on Media Technology and Applications},
	volume={7},
	number={2},
	pages={46--59},
	year={2019},
	publisher={The Institute of Image Information and Television Engineers}
}

@incollection{clefbook2018,
	Author = {Luca Piras and Barbara Caputo and Duc-Tien Dang-Nguyen and Michael Riegler and P\aa{} Halvorsen},
	Booktitle = {CLEF@20 - Information Retrieval Evaluation in a Changing World: Lessons Learned from 20 Years of CLEF},
	Publisher = {Springer Publishing Company},
	Title = {Image retrieval evaluation in specific domains},
	Year = 2018}

@article{dangnguyen2017multimodalretrieval,
	Acmid = {3103613},
	Address = {New York, NY, USA},
	Articleno = {49},
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Giacinto, Giorgio and Boato, Giulia and De Natale, Francesco GB},
	Date-Modified = {2017-09-18 14:39:22 +0000},
	Doi = {10.1145/3103613},
	Issn = {1551-6857},
	Issue_Date = {August 2017},
	Journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	Keywords = {Diversification, tourist attraction images retrieval},
	Month = aug,
	Number = {4},
	Numpages = {24},
	Pages = {49:1--49:24},
	Publisher = {ACM},
	Title = {{Multimodal Retrieval with Diversification and Relevance Feedback for Tourist Attraction Images}},
	Url = {http://doi.acm.org/10.1145/3103613},
	Volume = {13},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3103613},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3103613}}

@article{boididou2017,
	Author = {Boididou, Christina and Middleton, Stuart~E. and Jin, Zhiwei and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Kompatsiaris, Yiannis},
	Date-Modified = {2017-09-18 15:19:16 +0000},
	Journal = {Journal of Multimedia Tools and Applications},
	Title = {A Comparative Study of Automated Approaches for Verifying Information with Multimedia Content on Twitter},
	Year = {2017}}

@article{dangnguyen2015_cgvsreal3d,
	Abstract = {Modern computer graphics technologies brought realism in computer-generated characters, making them achieve truly natural appearance. Besides traditional virtual reality applications such as avatars, games, or cinema, these synthetic characters may be used to generate realistic fakes, which may lead to improper use of the technology. This fact raises the demand for advanced tools able to discriminate real and artificial human faces in digital media. In this paper, we propose a method to distinguish between computer generated and natural faces by modeling and evaluating their dynamic behavior. Because of a 3D-model-based video analysis, the proposed technique allows identifying synthetic characters by detecting their more limited variability over time. Experimental results demonstrate the effectiveness of the proposed approach also on very challenging and realistic video sequences.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and Francesco G. B. De Natale},
	Date-Modified = {2017-09-18 15:15:03 +0000},
	Doi = {10.1109/TIFS.2015.2427778},
	Issn = {1556-6013},
	Journal = {IEEE Transactions on Information Forensics and Security},
	Keywords = {Computer generated versus natural, facial analysis, video forensics},
	Number = {8},
	Pages = {1752--1763},
	Publisher = {IEEE},
	Title = {{3D-Model-Based Video Analysis for Computer Generated Faces Identification}},
	Volume = {10},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIFS.2015.2427778}}

@article{boato2015exploiting,
	Abstract = {Diversification of search results allows for better and faster search, gaining knowledge about different perspectives and viewpoints on retrieved information sources. Recently various methods for diversification of image retrieval results have been proposed, mainly using textual information or techniques imported from the natural language processing domain. However, images contain much more information than their textual descriptions and the use of visual features deserves special attention in this context. Visual saliency provides information about parts of the image perceived as most important, which are instinctively targeted by humans when shooting a photo or looking at a picture. For this reason we propose to exploit such information to improve diversification of search results. To this purpose, we introduce a saliency-based method to re-rank the results of a query and we show that it can achieve significantly better performances as compared to the baseline approach. Experimental validation conducted on a number of queries applied to various datasets demonstrates the potential of the use of saliency information for the diversification of image retrieval results.},
	Author = {Boato, Giulia and Dang-Nguyen, Duc-Tien and Muratov, Oleg and Alajlan, Naif and De Natale, Francesco GB},
	Doi = {10.1007/s11042-015-2526-4},
	Issn = {1573-7721},
	Journal = {Journal of Multimedia Tools and Applications},
	Keywords = {Visual saliency, Content-based image retrieval, Diversity},
	Pages = {1--22},
	Title = {Exploiting visual saliency for increasing diversity of image retrieval results},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11042-015-2526-4}}

@article{dao2012robust,
	Abstract = {Analyzing personal photo albums for understanding the related events is an emerging trend. A reliable event recognition tool could suggest appropriate annotation of pictures, provide the context for single image classification and tagging, achieve automatic selection and summarization, ease organization and sharing of media among users. In this paper, a novel method for fast and reliable event-type classification of personal photo albums is presented. Differently from previous approaches, the proposed method does not process photos individually but as a whole, exploiting three main features, namely Saliency, Gist, and Time, to extract an event signature, which is characteristic for a specific event type. A highly challenging database containing more than 40.000 photos belonging to 19 diverse event-types was crawled from photo-sharing websites for the purpose of modeling and performance evaluation. Experimental results showed that the proposed approach meets superior classification accuracy with limited computational complexity.},
	Author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and De Natale, Francesco G B},
	Doi = {10.1007/s11042-012-1153-6},
	Issn = {1573-7721},
	Journal = {Journal of Multimedia Tools and Applications},
	Keywords = {Gist of the scene, Saliency map, Approximate string matching, Human vision system, Personal photo albums, Holistic approach},
	Pages = {1--29},
	Title = {Robust event discovery from photo collections using Signature Image Bases (SIBs)},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11042-012-1153-6}}


@inproceedings{thambawita2020pmdata,
	title={PMData: a sports logging dataset},
	author={Thambawita, Vajira and Hicks, Steven Alexander and Borgli, Hanna and Stensland, H{\aa}kon Kvale and Jha, Debesh and Svensen, Martin Kristoffer and Pettersen, Svein-Arne and Johansen, Dag and Johansen, H{\aa}vard Dagenborg and Pettersen, Susann Dahl and others},
	booktitle={Proceedings of the 11th ACM Multimedia Systems Conference},
	pages={231--236},
	year={2020}
}

@article{borgli2020hyperkvasir,
	title={HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy},
	author={Borgli, Hanna and Thambawita, Vajira and Smedsrud, Pia H and Hicks, Steven and Jha, Debesh and Eskeland, Sigrun L and Randel, Kristin Ranheim and Pogorelov, Konstantin and Lux, Mathias and Nguyen, Duc Tien Dang and others},
	journal={Scientific Data},
	volume={7},
	number={1},
	pages={1--14},
	year={2020},
	publisher={Nature Publishing Group}
}


@inproceedings{ionescu2020imageclef,
	title={ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications},
	author={Ionescu, Bogdan and M{\"u}ller, Henning and P{\'e}teri, Renaud and Dang-Nguyen, Duc-Tien and Zhou, Liting and Piras, Luca and Riegler, Michael and Halvorsen, P{\aa}l and Tran, Minh-Triet and Lux, Mathias and others},
	booktitle={European Conference on Information Retrieval},
	pages={533--541},
	year={2020},
	organization={Springer, Cham}
}

@article{nguyen2020active,
	title={An active learning framework for duplicate detection in SaaS platforms},
	author={Nguyen, Quy H and Nguyen, Dac and Dao, Minh-Son and Dang-Nguyen, Duc-Tien and Gurrin, Cathal and Nguyen, Binh T},
	year={2020}
}

@inproceedings{cuong2020framework,
	title={A Framework for Paper Submission Recommendation System},
	author={Cuong, Dinh V and Nguyen, Dac H and Huynh, Son and Huynh, Phong and Gurrin, Cathal and Dao, Minh-Son and Dang-Nguyen, Duc-Tien and Nguyen, Binh T},
	booktitle={Proceedings of the 2020 International Conference on Multimedia Retrieval},
	pages={393--396},
	year={2020}
}

@inproceedings{gurrin2020introduction,
	title={Introduction to the Third Annual Lifelog Search Challenge (LSC'20)},
	author={Gurrin, Cathal and Le, Tu-Khiem and Ninh, Van-Tu and Dang-Nguyen, Duc-Tien and J{\'o}nsson, Bj{\"o}rn {\TH}{\'o}r and Loko{\v{s}}, Jakub and H{\"u}rst, Wolfgang and Tran, Minh-Triet and Schoeffmann, Klaus},
	booktitle={Proceedings of the 2020 International Conference on Multimedia Retrieval},
	pages={584--585},
	year={2020}
}

@inproceedings{nguyen2020malware,
	title={Malware Detection Using System Logs},
	author={Nguyen, Nhu T and Pham, Thuy T and Dang, Tien X and Dao, Minh-Son and Dang-Nguyen, Duc-Tien and Gurrin, Cathal and Nguyen, Binh T},
	booktitle={Proceedings of the 2020 Intelligent on Intelligent Cross-Data Analysis and Retrieval Workshop},
	pages={9--14},
	year={2020}
}

@inproceedings{nguyen2020duplicate,
	title={Duplicate Identification Algorithms in SaaS Platforms},
	author={Nguyen, Dac and Nguyen, Quy H and Dao, Minh-Son and Dang-Nguyen, Duc-Tien and Gurrin, Cathal and Nguyen, Binh T},
	booktitle={Proceedings of the 2020 Intelligent on Intelligent Cross-Data Analysis and Retrieval Workshop},
	pages={33--38},
	year={2020}
}

@incollection{gurrin2020experiments,
	title={Experiments in Lifelog Organisation and Retrieval at NTCIR},
	author={Gurrin, Cathal and Joho, Hideo and Hopfgartner, Frank and Zhou, Liting and Albatal, Rami and Healy, Graham and Nguyen, Duc-Tien Dang},
	booktitle={Evaluating Information Retrieval and Access Tasks},
	pages={187--203},
	year={2020},
	publisher={Springer, Singapore}
}

@inproceedings{ionescu2020overview,
	title={Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications},
	author={Ionescu, Bogdan and M{\"u}ller, Henning and P{\'e}teri, Renaud and Abacha, Asma Ben and Datla, Vivek and Hasan, Sadid A and Demner-Fushman, Dina and Kozlovski, Serge and Liauchuk, Vitali and Cid, Yashin Dicente and others},
	booktitle={International Conference of the Cross-Language Evaluation Forum for European Languages},
	pages={311--341},
	year={2020},
	organization={Springer, Cham}
}

@inproceedings{ninh2020overview,
	title={Overview of ImageCLEF lifelog 2020: lifelog moment retrieval and sport performance lifelog},
	author={Ninh, Van-Tu and Le, Tu-Khiem and Zhou, Liting and Piras, Luca and Riegler, Michael and l Halvorsen, P and Tran, MT and Lux, M and Gurrin, Cathal and Dang-Nguyen, Duc-Tien},
	booktitle={CLEF2020 Working Notes, ser. CEUR Workshop Proceedings. Thessaloniki, Greece: CEURWS. org< http://ceur-ws. org},
	year={2020}
}

@article{smedsrud2020kvasir,
	title={Kvasir-Capsule, a video capsule endoscopy dataset},
	author={Smedsrud, Pia H and Gjestang, Henrik L and Nedrejord, Oda O and N{\ae}ss, Espen and Thambawita, Vajira and Hicks, Steven and Borgli, Hanna and Jha, Debesh and Berstad, Tor Jan Derek and Eskeland, Sigrun L and others},
	year={2020},
	publisher={OSF Preprints}
}

@inproceedings{ionescu2020overview,
	title={Overview of the ImageCLEF 2020},
	author={Ionescu, Bogdan and M{\"u}ller, Henning and P{\'e}teri, Renaud and Ben Abacha, Asma and Datla, Vivek and Hasan, Sadid A and Demner-Fushman, Dina and Kozlovski, Serge and Liauchuk, Vitali and Dicente Cid, Yashin and others},
	booktitle={Proceedings of international conference of the cross-language evaluation forum for European languages (CLEF 2020)},
	number={CONFERENCE},
	year={2020},
	organization={22-25 September 2020}
}

@inproceedings{gurrin2019test,
	title={A test collection for interactive lifelog retrieval},
	author={Gurrin, Cathal and Schoeffmann, Klaus and Joho, Hideo and Munzer, Bernd and Albatal, Rami and Hopfgartner, Frank and Zhou, Liting and Dang-Nguyen, Duc-Tien},
	booktitle={International Conference on Multimedia Modeling},
	pages={312--324},
	year={2019},
	organization={Springer, Cham}
}

@article{tu2019dcu,
	title={DCU team at the 2019 insight for wellbeing task: multimodal personal health lifelog data analysis},
	author={Tu-Khiem, Le and Van-Tu, Ninh and Liting, Zhou and Duc-Tien, Dang-Nguyen and Cathal, Gurrin},
	year={2019}
}

@article{hicks2019acm,
	title={ACM MM BioMedia 2019 Grand Challenge Overview},
	author={Hicks, Steven and Riegler, Michael and Smedsrud, Pia and Haugen, Trine B and Randel, Kristin Randheim and Pogorelov, Konstantin and Stensland, H{\aa}kon Kvale and Dang-Nguyen, Duc-Tien and Lux, Mathias and Petlund, Andreas and others},
	journal={ACM, New York, NY, USA},
	year={2019}
}

@inproceedings{hicks2019medico,
	title={Medico Multimedia Task at MediaEval 2019},
	author={Hicks, Steven and Halvorsen, P{\aa}l and Haugen, Trine B and Andersen, Jorunn M and Witczak, Oliwia and Pogorelov, Konstantin and Hammer, Hugo L and Dang-Nguyen, Duc-Tien and Lux, Mathias and Riegler, Michael},
	booktitle={Proceedings of the CEUR Workshop on Multimedia Benchmark Workshop (MediaEval)},
	year={2019}
}

@inproceedings{ionescu2019imageclef,
	title={Imageclef 2019: Multimedia retrieval in lifelogging, medical, nature, and security applications},
	author={Ionescu, Bogdan and M{\"u}ller, Henning and P{\'e}teri, Renaud and Dang-Nguyen, Duc-Tien and Piras, Luca and Riegler, Michael and Tran, Minh-Triet and Lux, Mathias and Gurrin, Cathal and Cid, Yashin Dicente and others},
	booktitle={European Conference on Information Retrieval},
	pages={301--308},
	year={2019},
	organization={Springer, Cham}
}

@article{lux2019summarizing,
	title={Summarizing E-Sports Matches and Tournaments},
	author={Lux, Mathias and Halvorsen, P{\aa}l and Dang-Nguyen, Duc-Tien and Stensland, H{\aa}kon and Kesavulu, Manoj and Potthast, Martin and Riegler, Michael},
	year={2019}
}

@inproceedings{ninh2019baseline,
	title={A baseline interactive retrieval engine for the nticr-14 lifelog-3 semantic access task},
	author={Ninh, Van-Tu and Le, Tu-Khiem and Zhou, Liting and Healy, Graham and Tran, Minh-Triet and Dang-Nguyen, Duc-Tien and Smyth, Sinead and Gurrin, Cathal},
	booktitle={The Fourteenth NTCIR Conference (NTCIR-14)},
	year={2019}
}

@inproceedings{le2019lifeseeker,
	title={LifeSeeker: Interactive Lifelog Search Engine at LSC 2019},
	author={Le, Tu-Khiem and Ninh, Van-Tu and Dang-Nguyen, Duc-Tien and Tran, Minh-Triet and Zhou, Liting and Redondo, Pablo and Smyth, Sinead and Gurrin, Cathal},
	booktitle={Proceedings of the ACM Workshop on Lifelog Search Challenge},
	pages={37--40},
	year={2019}
}

@inproceedings{lux2019summarizing,
	title={Summarizing E-sports matches and tournaments: the example of counter-strike: global offensive},
	author={Lux, Mathias and Halvorsen, P{\aa}l and Dang-Nguyen, Duc-Tien and Stensland, H{\aa}kon and Kesavulu, Manoj and Potthast, Martin and Riegler, Michael},
	booktitle={Proceedings of the 11th ACM Workshop on Immersive Mixed and Virtual Environment Systems},
	pages={13--18},
	year={2019}
}

@inproceedings{ionescu2019imageclef,
	title={ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature},
	author={Ionescu, Bogdan and M{\"u}ller, Henning and P{\'e}teri, Renaud and Cid, Yashin Dicente and Liauchuk, Vitali and Kovalev, Vassili and Klimuk, Dzmitri and Tarasau, Aleh and Abacha, Asma Ben and Hasan, Sadid A and others},
	booktitle={International Conference of the Cross-Language Evaluation Forum for European Languages},
	pages={358--386},
	year={2019},
	organization={Springer, Cham}
}

@inproceedings{dang2019overview,
	title={Overview of ImageCLEFlifelog 2019: solve my life puzzle and lifelog moment retrieval},
	author={Dang Nguyen, Duc Tien and Piras, Luca and Riegler, Michael and Zhou, Liting and Lux, Mathias and Tran, Minh Triet and Le, Tu-Khiem and Ninh, Van-Tu and Gurrin, Cathal},
	year={2019},
	organization={CEUR Workshop Proceedings}
}

@inproceedings{van2019lifer,
	title={Lifer 2.0: Discovering personal lifelog insights using an interactive lifelog retrieval system},
	author={Van, Tu-ninh and Le, Tu-Khiem and Zhou, Liting and Piras, Luca and Riegler, Michael and Lux, Mathias and Tran, Minh-Triet and Gurrin, Cathal and Dang Nguyen, Duc Tien},
	year={2019},
	organization={CLEF}
}

@inproceedings{gurrin2019advances,
	title={Advances in lifelog data organisation and retrieval at the ntcir-14 lifelog-3 task},
	author={Gurrin, Cathal and Joho, Hideo and Hopfgartner, Frank and Zhou, Liting and Ninh, Van-Tu and Le, Tu-Khiem and Albatal, Rami and Dang-Nguyen, Duc-Tien and Healy, Graham},
	booktitle={NII Conference on Testbeds and Community for Information Access Research},
	pages={16--28},
	year={2019},
	organization={Springer, Cham}
}

@incollection{lux2019challenges,
	title={Challenges for Multimedia Research in E-Sports Using Counter-Strike},
	author={Lux, Mathias and Riegler, Michael and Halvorsen, Pal and Dang-Nguyen, Duc-Tien and Potthast, Martin},
	booktitle={Savegame},
	pages={197--206},
	year={2019},
	publisher={Springer VS, Wiesbaden}
}

@inproceedings{ninh2019baseline,
	title={A Baseline Interactive Retrieval Engine for Visual Lifelogs at the NTCIR-14 Lifelog-3 Task},
	author={Ninh, Van-Tu and Le, Tu-Khiem and Zhou, Liting and Healy, Graham and Venkataraman, Kaushik and Tran, Minh-Triet and Dang-Nguyen, Duc-Tien and Smyth, Sinead and Gurrin, Cathal},
	booktitle={NII Conference on Testbeds and Community for Information Access Research},
	pages={29--41},
	year={2019},
	organization={Springer, Cham}
}





@article{hicks2019predicting,
	title={Predicting Sperm Motility and Morphology using Deep Learning and Handcrafted Features},
	author={Hicks, Steven and Halvorsen, P{\aa}l and Haugen, Trine B and Andersen, Jorunn M and Witczak, Oliwia and Hammer, Hugo L and Dang-Nguyen, Duc-Tien and Lux, Mathias and Riegler, Michael},
	year={2019}
}

@article{ninh2019replay,
	title={Replay detection and multi-stream synchronization in CS: GO game streams using content-based Image retrieval and Image signature matching},
	author={Ninh, Van-Tu and Le, Tu-Khiem and Dang-Nguyen, Duc-Tien and Gurrin, Cathal},
	year={2019},
	publisher={CEUR-WS}
}



@inproceedings{ionescu2018overview,
	Author = {Ionescu, Bogdan and M{\"u}ller, Henning and Villegas, Mauricio and de Herrera, Alba Garc{\'\i}a Seco and Eickhoff, Carsten and Andrearczyk, Vincent and Cid, Yashin Dicente and Liauchuk, Vitali and Kovalev, Vassili and Hasan, Sadid A and others},
	Booktitle = {International Conference of the Cross-Language Evaluation Forum for European Languages},
	Organization = {Springer, Cham},
	Pages = {309--334},
	Title = {Overview of ImageCLEF 2018: Challenges, datasets and evaluation},
	Year = {2018}}

@inproceedings{dang2018overview,
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Riegler, Michael and Zhou, Liting and Lux, Mathias and Gurrin, Cathal},
	Booktitle = {CLEF2018 Working Notes. CEUR Workshop Proceedings, CEURWS. org< http://ceur-ws. org>, Avignon, France (September 10-14 2018)},
	Title = {Overview of ImageCLEFlifelog 2018: daily living understanding and lifelog moment retrieval},
	Year = {2018}}

@inproceedings{zhou2018interactive,
	Author = {Zhou, Liting and Piras, Luca and Riegler, Michael and Lux, Mathias and Dang-Nguyen, Duc-Tien and Gurrin, Cathal},
	Booktitle = {19th Working Notes of CLEF Conference and Labs of the Evaluation Forum, CLEF 2018},
	Organization = {CEUR-WS},
	Title = {An Interactive Lifelog Retrieval System for Activities of Daily Living Understanding},
	Volume = {2125},
	Year = {2018}}

@inproceedings{gurrin2018test,
	Author = {Gurrin, C and Schoeffmann, K and Joho, H and Munzer, B and Albatal, R and Hopfgartner, F and Zhou, L and Dang-Nguyen, D-T},
	Booktitle = {Proceedings of 25th International Conference on MultiMedia Modeling},
	Organization = {Springer Verlag},
	Title = {A test collection for interactive lifelog retrieval},
	Year = {2018}}

@inproceedings{pogorelov2018medico,
	Author = {Pogorelov, Konstantin and Riegler, Michael and Halvorsen, P{\aa}l and De Lange, Thomas and Randel, Kristin Ranheim and Dang-Nguyen, Duc-Tien and Lux, Mathias and Ostroukhova, Olga},
	Booktitle = {Working Notes Proceedings of the MediaEval 2018 Workshop},
	Title = {Medico Multimedia Task at MediaEval 2018},
	Year = {2018}}

@inproceedings{lux2018gamestory,
	Author = {Lux, Mathias and Riegler, Michael and Dang-Nguyen, Duc-Tien and Larson, Marcus and Potthast, Martin and Halvorsen, P{\aa}l},
	Booktitle = {Working Notes Proceedings of the MediaEval 2018 Workshop},
	Title = {GameStory Task at MediaEval 2018},
	Year = {2018}}

@inproceedings{ostroukhova2018transfer,
	Author = {Ostroukhova, Olga and Pogorelov, Konstantin and Riegler, Michael and Dang-Nguyen, Duc-Tien and Halvorsen, P{\aa}l},
	Booktitle = {Working Notes Proceedings of the MediaEval 2018 Workshop},
	Title = {Transfer learning with prioritized classification and training dataset equalization for medical objects detection},
	Year = {2018}}

@inproceedings{lux2018team,
	Author = {Lux, Mathias and Riegler, Michael and Dang-Nguyen, Duc-Tien and Larson, Marcus and Potthast, Martin and Halvorsen, P{\aa}l},
	Booktitle = {Working Notes Proceedings of the MediaEval 2018 Workshop},
	Title = {Team ORG@ GameStory Task 2018},
	Year = {2018}}

@inproceedings{dangnguyen2018icmr,
	_Note = {Accepted},
	Author = {Duc-Tien Dang-Nguyen and Michael Riegler and Liting Zhou and Cathal Gurrin},
	Booktitle = {ACM International Conference on Multimedia Retrieval},
	Title = {{Challenges and Opportunities within Personal Life Archives}},
	Year = {2018}}

@inproceedings{dcu2018lsc,
	_Note = {Accepted},
	Author = {Zhou, Liting and Zaher Hinbarji and Dang-Nguyen, Duc-Tien and Cathal Gurrin},
	Booktitle = {Workshop on Lifelog Search Challenge},
	Title = {{LIFER: An Interactive Lifelog Retrieval System}},
	Year = {2018}}

@inproceedings{dangnguyen2018lscpanel,
	_Note = {Accepted},
	Author = {Dang-Nguyen, Duc-Tien and Klaus Schoeffmann and Wolfgang Hurst},
	Booktitle = {Workshop on Lifelog Search Challenge},
	Title = {{LSC2018 Panel - Challenges of Lifelog Search And Access}},
	Year = {2018}}

@inproceedings{dangnguyen2018usageanalytics,
	Author = {Dang-Nguyen, Duc-Tien and Manoj Kesavulu and Markus Helfert},
	Booktitle = {International Conference on Smart Cities and Green ICT Systems (SMARTGREENS)},
	Title = {{Usage Analytics: Research Directions to Discover Insights from Cloud-based Applications}},
	Year = {2018}}

@inproceedings{kesavulu2018usagemonitoring,
	_Note = {Accepted},
	Author = {Manoj Kesavulu and Duc-Tien Dang-Nguyen and Markus Helfert and Marija Bezbradica},
	Booktitle = {UK Academy for Information Systems International Conference},
	Title = {{An Overview of User-level Usage Monitoring in Cloud Environment}},
	Year = {2018}}

@inproceedings{dao2018ICPRAM,
	Author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and Kasem, Asem and Tran-The, Hung},
	Booktitle = {International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
	Title = {{HealthyClassroom - A Proof-of-Concept Study for Discovering Students' Daily Moods and Classroom Emotions to Enhance a Learning-teaching Process using Heterogeneous Sensors}},
	Year = {2018}}

@inproceedings{binh2018ICPRAM,
	Author = {Nguyen, T. Binh and Dang-Nguyen, Duc-Tien and Tien X. Dang and Thai Phat and Gurrin, Cathal},
	Booktitle = {International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
	Note = {Accepted},
	Title = {{A Deep Learning based Food Recognition System for Lifelog Images}},
	Year = {2018}}

@inproceedings{Gurrin2017NTCIR-Overview,
	Author = {Cathal Gurrin and Hideo Joho and Frank Hopfgartner and Liting Zhou and Rashmi Gupta and Rami Albatal and Duc-Tien Dang-Nguyen},
	Booktitle = {Proceedings of the 13th NTCIR Conference on Evaluation of Information Access Technologies},
	Title = {{Overview of NTCIR-13 Lifelog-2 Task}},
	Year = {2017}}

@inproceedings{Zhou2017DCUteam,
	Author = {Liting Zhou and Aaron Duane and Duc-Tien Dang-Nguyen and Cathal Gurrin},
	Booktitle = {Proceedings of the 13th NTCIR Conference on Evaluation of Information Access Technologies},
	Title = {{DCU at the NTCIR-13 Lifelog-2 Task}},
	Year = {2017}}

@inproceedings{gurrin2017LTAoverview,
	Author = {Cathal Gurrin and Xavier Giro-i-Nieto and Petia Radeva and Mariella Dimiccoli and Duc-Tien Dang-Nguyen and Hideo Joho},
	Booktitle = {ACM Workshop on Lifelogging Tools and Applications (LTA)},
	Date-Added = {2017-09-19 14:57:56 +0000},
	Date-Modified = {2017-09-19 14:59:16 +0000},
	Title = {{LTA 2017: The Second Workshop on Lifelogging Tools and Applications}},
	Year = {2017}}

@inproceedings{zhou2017baselinesearch,
	Author = {Liting Zhou and Duc-Tien Dang-Nguyen and Cathal Gurrin},
	Booktitle = {ACM Workshop on Lifelogging Tools and Applications (LTA)},
	Date-Added = {2017-09-19 10:31:53 +0000},
	Date-Modified = {2017-09-19 10:34:52 +0000},
	Title = {A Baseline Search Engine for Personal Life Archive},
	Year = {2017}}

@inproceedings{pogorelov2017MedioComparison,
	Author = {Konstantin Pogorelov and Michael Riegler and P{\aa}l Halvorsen and Carsten Griwodz and Thomas de Lange and Kristin Ranheim Randel and Sigrun Losada Eskeland and Duc-Tien Dang-Nguyen and Olga Ostroukhova and Mathias Lux and Concetto Spampinato},
	Booktitle = {MediaEval 2017 Multimedia Benchmark Workshop},
	Date-Added = {2017-09-18 15:24:06 +0000},
	Date-Modified = {2017-09-18 15:25:28 +0000},
	Month = {September 14-16},
	Title = {A Comparison of Deep Learning with Global Features for Gastrointestinal Disease Detection},
	Year = {2017}}

@inproceedings{Riegler2017MedicoTask,
	Author = {Michael Riegler and Konstantin Pogorelov and P{\aa}l Halvorsen and Carsten Griwodz and Thomas de Lange and Kristin Ranheim Randel and Sigrun Losada Eskeland and Duc-Tien Dang-Nguyen and Mathias Lux and Concetto Spampinato},
	Booktitle = {MediaEval 2017 Multimedia Benchmark Workshop},
	Date-Added = {2017-09-18 15:22:03 +0000},
	Date-Modified = {2017-09-18 15:23:22 +0000},
	Month = {September 14-16},
	Title = {Multimedia for Medicine: The Medico Task at MediaEval 2017},
	Year = {2017}}

@inproceedings{dao2017satellite,
	Author = {Minh-Son Dao and Quang-Nhat-Minh Pham and Duc-Tien Dang-Nguyen},
	Booktitle = {MediaEval 2017 Multimedia Benchmark Workshop},
	Date-Added = {2017-09-18 15:12:20 +0000},
	Date-Modified = {2017-09-18 15:23:39 +0000},
	Month = {September 14-16},
	Title = {A Domain-based Late-Fusion for Disaster Image Retrieval from Social Media},
	Year = {2017}}

@inproceedings{phan2017LDP-TOPiciap,
	Author = {Quoc-Tin Phan and Duc-Tien Dang-Nguyen and Giulia Boato and Francesco G. B. De Natale},
	Booktitle = {GIRPR International Conference on Image Analysis and Processing},
	Date-Added = {2017-09-18 14:49:48 +0000},
	Date-Modified = {2017-09-18 14:52:49 +0000},
	Title = {{Using LDP-TOP in Video-Based Spoofing Detection}},
	Year = {2017}}

@inproceedings{zhou2017LifelogOrganizers,
	Address = {Dublin, Ireland},
	Author = {Liting Zhou and Luca Piras and Michael Rieger and Giulia Boato and Duc-Tien Dang-Nguyen and Cathal Gurrin},
	Booktitle = {CLEF 2017},
	Date-Modified = {2017-09-18 15:19:26 +0000},
	Journal = {{CLEF} working notes, CEUR},
	Month = {September 11-14},
	Publisher = {CEUR-WS.org $<$http://ceur-ws.org$>$},
	Title = {Organizer Team at ImageCLEFlifelog 2017: Baseline Approaches for Lifelog Retrieval and Summarization},
	Year = {2017}}

@inbook{Ionescu2017,
	Abstract = {This paper presents an overview of the ImageCLEF 2017 evaluation campaign, an event that was organized as part of the CLEF (Conference and Labs of the Evaluation Forum) labs 2017. ImageCLEF is an ongoing initiative (started in 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval for providing information access to collections of images in various usage scenarios and domains. In 2017, the 15th edition of ImageCLEF, three main tasks were proposed and one pilot task: (1) a LifeLog task about searching in LifeLog data, so videos, images and other sources; (2) a caption prediction task that aims at predicting the caption of a figure from the biomedical literature based on the figure alone; (3) a tuberculosis task that aims at detecting the tuberculosis type from CT (Computed Tomography) volumes of the lung and also the drug resistance of the tuberculosis; and (4) a remote sensing pilot task that aims at predicting population density based on satellite images. The strong participation of over 150 research groups registering for the four tasks and 27 groups submitting results shows the interest in this benchmarking campaign despite the fact that all four tasks were new and had to create their own community.},
	Address = {Cham},
	Author = {Ionescu, Bogdan and M{\"u}ller, Henning and Villegas, Mauricio and Arenas, Helbert and Boato, Giulia and Dang-Nguyen, Duc-Tien and Dicente Cid, Yashin and Eickhoff, Carsten and Seco de Herrera, Alba G. and Gurrin, Cathal and Islam, Bayzidul and Kovalev, Vassili and Liauchuk, Vitali and Mothe, Josiane and Piras, Luca and Riegler, Michael and Schwall, Immanuel},
	Booktitle = {Experimental IR Meets Multilinguality, Multimodality, and Interaction: 8th International Conference of the CLEF Association, CLEF 2017, Dublin, Ireland, September 11--14, 2017, Proceedings},
	Doi = {10.1007/978-3-319-65813-1_28},
	Editor = {Jones, Gareth J.F. and Lawless, S{\'e}amus and Gonzalo, Julio and Kelly, Liadh and Goeuriot, Lorraine and Mandl, Thomas and Cappellato, Linda and Ferro, Nicola},
	Isbn = {978-3-319-65813-1},
	Pages = {315--337},
	Publisher = {Springer International Publishing},
	Title = {Overview of ImageCLEF 2017: Information Extraction from Images},
	Url = {https://doi.org/10.1007/978-3-319-65813-1_28},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-319-65813-1_28}}

@inproceedings{dangnguyen2017LifeLogTask17,
	Address = {Dublin, Ireland},
	Author = {Duc-Tien Dang-Nguyen and Luca Piras and Michael Riegler and Giulia Boato and Liting Zhou and Cathal Gurrin},
	Booktitle = {CLEF 2017},
	Date-Modified = {2017-09-18 15:17:58 +0000},
	Month = {September 11-14},
	Publisher = {CEUR-WS.org $<$http://ceur-ws.org$>$},
	Series = {{CEUR} Workshop Proceedings},
	Title = {{Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization}},
	Year = {2017}}

@inproceedings{dangnguyen2017dataset,
	_Note = {Accepted},
	Abstract = {In this paper, we address the challenge of how to build a disclosed lifelog dataset by proposing the principles for building and sharing such types of data. Based on the proposed principles, we describe processes for how we built the benchmarking lifelog dataset for NTCIR-13 - Lifelog 2 tasks. Further, a list of potential applications and a framework for anonymisation are proposed and discussed.},
	Author = {Dang-Nguyen, Duc-Tien and Zhou, Liting and Gupta, Rashmi and Riegler, Michael and Gurrin, Cathal},
	Booktitle = {Content-Based Multimedia Indexing (CBMI)},
	Title = {{Building a Disclosed Lifelog Dataset: Challenges,~Principles~and~Processes}},
	Year = {2017}}

@inproceedings{ahmad2017JORD,
	_Note = {Accepted},
	Author = {Kashif Ahmad and Michael Riegler and Ans Riaz and Nicola Conci and Duc-Tien Dang-Nguyen and P{\aa}l Halvorsen},
	Booktitle = {ACM International Conference on Multimedia Retrieval},
	Date-Modified = {2017-09-18 15:15:36 +0000},
	Title = {{The JORD System - Linking Sky and Social Multimedia Data to Natural and Technological Disasters}},
	Year = {2017}}

@inproceedings{konstantin2017,
	Author = {Konstantin Pogorelov and Kristin Ranheim Randel and Carsten Griwodz and Thomas de Lange and Sigrun Losada Eskeland and Dag Johansen and Concetto Spampinato and Dang-Nguyen, Duc-Tien and Mathias Lux and Peter Thelin Schmidt and Riegler, Michael and P{\aa}l Halvorsen},
	Booktitle = {ACM Multimedia Systems},
	Date-Modified = {2017-09-19 10:33:49 +0000},
	Title = {{Kvarsir: A Multi-Class Image-Dataset for Computer Aided Gastrointestinal Disease Detection}},
	Year = {2017}}

@inproceedings{konstantin2017hollistic,
	_Note = {Accepted},
	Author = {Konstantin Pogorelov and Carsten Griwodz and Sigrun Losada and Duc-Tien Dang-Nguyen and H{\aa}kon Stensland and Thomas de Lange and Dag Johansen and Francesco De~Natale and Kristin Ranheim and Riegler, Michael and P{\aa}l Halvorsen},
	Booktitle = {ACM Multimedia Systems},
	Date-Modified = {2017-09-19 10:33:53 +0000},
	Title = {{A Holistic Multimedia System for Gastrointestinal Tract Disease Detection}},
	Year = {2017}}

@inproceedings{dao2017,
	Author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and Riegler, Michael and Gurrin, Cathal},
	Booktitle = {International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
	Date-Modified = {2017-09-19 10:34:36 +0000},
	Title = {{Smart Lifelogging: Recognizing Human~Activities using PHASOR}},
	Year = {2017}}

@inproceedings{boididou2016vmu,
	Author = {Boididou, Christina and Middleton, Stuart E and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Riegler, Michael and Boato, Giulia and Petlund, Andreas and Kompatsiaris, Yiannis},
	Booktitle = {MediaEval 2016 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:13:41 +0000},
	Publisher = {CEUR-WS},
	Title = {{The VMU Participation@ Verifying Multimedia Use 2016}},
	Year = {2016}}

@misc{dangnguyen2016NIPS,
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Riegler, Michael and Gurrin, Cathal and Boato, Giulia and Harvosen, Paal},
	Booktitle = {Challenges in Machine Learning: Gaming and Education NIPS 2016 workshop},
	Date-Modified = {2017-09-18 14:41:02 +0000},
	Title = {{ImageCLEF 2017 LifeLog task}},
	Year = {2016}}

@inproceedings{boididou2016verifying,
	Author = {Boididou, Christina and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Riegler, Michael and Middleton, Stuart E. and Petlund, Andreas and Kompatsiaris, Yiannis},
	Booktitle = {MediaEval 2016 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:13:50 +0000},
	Title = {Verifying multimedia use at MediaEval 2016},
	Year = {2016}}

@inproceedings{riegler2016mmsys,
	Abstract = {In this paper, we present Heimdallr, a dataset that aims to serve two different purposes. The first purpose is action recognition and pose estimation, which requires a dataset of annotated sequences of athlete skeletons. We employed a crowdsourcing platform where people around the world were asked to annotate frames and obtained more than $3000$ fully annotated frames for $42$ different sequences with a variety of poses and actions. The second purpose is an improved understanding of crowdworkers, and for this purpose, we collected over $10000$ written feedbacks from $592$ crowdworkers. 	This is valuable information for crowdsourcing researchers who explore 	algorithms for worker quality assessment. In addition to the complete dataset, we also provide the code for the application that has been used to collect the data as an open source software.},
	Author = {Michael Riegler and Duc-Tien Dang-Nguyen and B\r{a}rd Winther and Carsten Griwodz and	Konstantin Pogorelov and P\r{a}l Halvorsen},
	Booktitle = {ACM Multimedia Systems},
	Date-Modified = {2017-09-18 15:15:48 +0000},
	Title = {Heimdallr: A dataset for sport analysis},
	Year = 2016}

@inproceedings{phan2016LDP-TOP,
	Abstract = {In this paper, we propose a novel approach for face spoofing detection using the high-order Local Derivative Pattern from Three Orthogonal Planes (LDP-TOP). The proposed method is not only simple to derive and implement, but also highly efficient, since it takes into account both spatial and temporal information in different directions of subtle face movements. According to experimental results, the proposed approach outperforms state-of-the-art methods on three reference datasets, namely Idiap REPLAY-ATTACK, CASIAFASD, and MSU MFSD. Moreover, it requires only 25 video frames from each video, i.e., only one second, and thus potentially can be performed in real time even on low-cost devices.},
	Author = {Phan, Quoc-Tin and Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Conference on Image Processing},
	Date-Modified = {2017-09-18 15:16:02 +0000},
	Keywords = {Face Anti-Spoofing, Local Derivative Pattern, Video Forensics},
	Title = {Face Spoofing Detection Using LDP-TOP},
	Year = 2016}

@inproceedings{dangnguyen2015hybrid,
	Abstract = {In this paper, we present a novel method that can produce a visual description of a landmark by choosing the most diverse pictures that best describe all the details of the queried location from community-contributed datasets. The main idea of this method is to filter out non-relevant images at a first stage and then cluster the images according to textual descriptors first, and then to visual descriptors. The extraction of images from different clusters according to a measure of user's credibility, allows obtaining a reliable set of diverse and relevant images. Experimental results performed on the MediaEval 2014 ``Retrieving Diverse Social Images'' dataset show that the proposed approach can achieve very good performance outperforming state-of-art techniques.},
	Annote = {abc},
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Giacinto, Giorgio and Boato, Giulia and De Natale, Francesco GB},
	Booktitle = {IEEE International Conference on Multimedia and Expo (ICME)},
	Date-Modified = {2019-02-18 20:14:36 +0000},
	Doi = {10.1109/ICME.2015.7177486},
	Keywords = {Social Image Retrieval, Diversity},
	Pages = {1--6},
	Title = {A hybrid approach for retrieving diverse social images of landmarks},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICME.2015.7177486}}

@inproceedings{dangnguyen2015multimodal,
	Abstract = {In this paper, we describe our approach and its results for the MediaEval 2015 Retrieving Diverse Social Images task. The main strength of the proposed approach is its flexibility that permits to filter out irrelevant images, and to obtain a reli- able set of diverse and relevant images. This is done by first clustering similar images according to their textual descrip- tions and their visual content, and then extracting images from different clusters according to a measure of user's cred- ibility. Experimental results shown that it is stable and has little fluctuation in both single-concept and multi-concept queries.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and Natale, Francesco G and Piras, Luca and Giacinco, Giorgio and Tuveri, Franco and Angioni, Manuela},
	Booktitle = {MediaEval 2015 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:20:06 +0000},
	Title = {Multimodal-based Diversified Summarization in Social Image Retrieval},
	Volume = {1436},
	Year = {2015}}

@inproceedings{boididou2015verifying,
	Abstract = {This paper provides an overview of the Verifying Multimedia Use task that takes places as part of the 2015 MediaEval Benchmark. The task deals with the automatic detection of manipulation and misuse of Web multimedia content. Its aim is to lay the basis for a future generation of tools that could assist media professionals in the process of verifica- tion. Examples of manipulation include maliciously tamper- ing with images and videos, e.g., splicing, removal/addition of elements, while other kinds of misuse include the reposting of previously captured multimedia content in a different con- text (e.g., a new event) claiming that it was captured there. For the 2015 edition of the task, we have generated and made available a large corpus of real-world cases of images that were distributed through tweets, along with manually assigned labels regarding their use, i.e. misleading (fake) versus appropriate (real).},
	Author = {Boididou, Christina and Andreadou, Katerina and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Riegler, Michael and Kompatsiaris, Yiannis},
	Booktitle = {MediaEval 2015 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:20:12 +0000},
	Title = {Verifying multimedia use at MediaEval 2015},
	Volume = {1436},
	Year = {2015}}

@inproceedings{boididou2015certh,
	Abstract = {We propose an approach that predicts whether a tweet, which is accompanied by multimedia content (image/video), is trustworthy or deceptive. We test different combinations of quality and trust-oriented features (tweet-based, user- based and forensics) in tandem with a standard classification and an agreement-retraining technique, with the goal of pre- dicting the most likely label (fake or real) for each tweet. The experiments carried out on the Verifying Multimedia Use dataset show that the best performance is achieved when using all available features in combination with the agreement-retraining method.},
	Author = {Boididou, Christina and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Kompatsiaris, Yiannis},
	Booktitle = {MediaEval 2015 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:20:20 +0000},
	Title = {{The CERTH-UNITN Participation@ Verifying Multimedia Use 2015}},
	Volume = {1436},
	Year = {2015}}

@inproceedings{dangnguyen2015raise,
	Abstract = {Digital forensics is a relatively new research area which aims at authenticating digital media by detecting possible digital forgeries. Indeed, the ever increasing availability of multimedia data on the web, coupled with the great advances reached by computer graphical tools, makes the modification of an image and the creation of visually compelling forgeries an easy task for any user. This in turns creates the need of reliable tools to validate the trustworthiness of the represented information. In such a context, we present here RAISE, a large dataset of 8156 high-resolution raw images, depicting various subjects and scenarios, properly annotated and available together with accompanying metadata. Such a wide collection of untouched and diverse data is intended to become a powerful resource for, but not limited to, forensic researchers by providing a common benchmark for a fair comparison, testing and evaluation of existing and next generation forensic algorithms. In this paper we describe how RAISE has been collected and organized, discuss how digital image forensics and many other multimedia research areas may benefit of this new publicly available benchmark dataset and test a very recent forensic technique for JPEG compression detection.},
	Acmid = {2713194},
	Author = {Dang-Nguyen, Duc-Tien and Pasquini, Cecilia and Conotter, Valentina and Boato, Giulia},
	Booktitle = {ACM Multimedia Systems},
	Doi = {10.1145/2713168.2713194},
	Isbn = {978-1-4503-3351-1},
	Keywords = {Data Set, Raw Images, Benchmark, Image Forensics},
	Pages = {219--224},
	Publisher = {ACM},
	Title = {{RAISE -- A Raw Images Dataset for Digital Image Forensics}},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2713168.2713194}}

@inproceedings{dangnguyen2014facedynamics,
	Abstract = {Digital graphics tools are nowadays capable of rendering highly photorealistic imagery, which easily puzzle our perception of reality. This poses serious ethical and legal issues, which in turn create the need for further technologies able to ensure the trustworthiness of digital media as a true representation of reality, especially when depicting humans. In this work, we propose a novel forensic technique to tackle the problem of distinguishing computer generated (CG) from real humans in videos. It exploits the temporal information inherent of a video sequence by analyzing the spatio-temporal appearance of facial expressions in both CG and real humans. Even if rendering facial expression has reached outstanding performances, CG face appearance over time still presents some underlying mechanical properties that greatly differ from the natural muscle movements of real humans. We build an efficient classifier on a set of features describing facial dynamics and spatio-temporal changes during smiling to distinguish CG from human faces. Experimental results demonstrate the effectiveness of the proposed approach.},
	Author = {Dang-Nguyen, Duc-Tien and Conotter, Valentina and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Workshop on Information Forensics and Security (WIFS)},
	Doi = {10.1109/WIFS.2014.7084321},
	Pages = {161--166},
	Title = {Video forensics based on expression dynamics},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/WIFS.2014.7084321}}

@inproceedings{conotter2014deception,
	Abstract = {We present a crowdsourcing approach to tackle the challenge of collecting hard-to-find data. Our immediate need for the data arises because we are studying edited images in context online, and the way that this use impacts users' perceptions. Study of this topic cannot advance without a large, diverse data set of image/context pairs. The image in the pair should be suspected of having been edited, and the context is the place (e.g., website or social media post) in which it has been used online. Such pairs are hard to find, and could not be collected, due to techno-practical constraints, without the support of crowdsourcing. This paper describes a three-step approach to data set creation involving mining social data, applying image analysis techniques, and, finally, making use of the crowd to complete the necessary information. We close with a discussion of the potential and limitations of the data set collected.},
	Author = {Conotter, Valentina and Dang-Nguyen, Duc-Tien and Riegler, Michael and Boato, Guilia and Larson, Martha},
	Booktitle = {ACM Internaltional Workshop on Crowdsourcing for Multimedia},
	Date-Modified = {2017-09-18 15:20:38 +0000},
	Doi = {10.1145/2660114.2660120},
	Isbn = {978-1-4503-3128-9},
	Keywords = {Human Perception; Edited Images; Test design; Data Set},
	Pages = {49--52},
	Title = {A Crowdsourced Data Set of Edited Images Online},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2660114.2660120}}

@inproceedings{dangnguyen2014Deformation,
	Abstract = {Given the recent development of advanced multimedia techniques able to support the creation of realistic computer generated characters, there is the parallel need of automatic tools allowing users to verify the source of the multimedia data they are observing, thus discriminating between artificial and natural information. In this paper, we focus on video representing human beings and we propose a novel method to identify computer generated characters by analysing the evolution of the face model in chronological order. Experimental results show that photorealistic facial animations, which are usually performed following fixed patterns, can be distinguished from natural ones, which follow much more complicated and various geometric distortions.},
	Annotation = {conference},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Conference on Image Processing},
	Date-Modified = {2017-09-18 15:20:49 +0000},
	Doi = {10.1109/ICIP.2014.7026078},
	Pages = {5327--5331},
	Title = {{Revealing Synthetic Facial Animations of Realistic Characters}},
	Year = 2014,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICIP.2014.7026078}}

@inproceedings{dangnguyen2014mediaeval,
	Abstract = {In this paper, we describe our approach and its results for MediaEval 2014 Retrieving Diverse Social Images Task. The basic idea of our proposed method is to filter out non-relevant images at the beginning of the process and then construct a hierarchical tree which allows to cluster the images with different criteria on visual and textual features. Experimen- tal results shown that it is stable and has little fluctuation with the number of topics.},
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Giacinto, Giorgio and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {MediaEval 2014 Multimedia Benchmark Workshop},
	Note = {{WINNER}},
	Title = {Retrieval of Diverse Images by Pre-filtering and Hierarchical Clustering},
	Volume = {1263},
	Year = 2014}

@misc{rosani2014eventmask,
	Author = {Rosani, Andrea and Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Date-Modified = {2017-09-18 14:37:52 +0000},
	Keywords = {Event Detection, Gaming, Saliency, Photo Galleries},
	Organization = {IEEE International Conference on Acoustics, Speech and Signal Processing, Show and Tell Demo},
	Title = {EventMask: a game-based analysis of event-related saliency in photo galleries},
	Year = {2014}}

@inproceedings{conotter2014crowd,
	Abstract = {Generally, we expect images to be an honest reflection of reality. However, this assumption is undermined by the new image editing technology, which allows for easy manipulation and distortion of digital contents. Our understanding of the implications related to the use of a manipulated data is lagging behind. In this paper we propose to exploit crowdsourcing tools in order to analyze the impact of different types of manipulation on users' perceptions of deception. Our goal is to gain significant insights about how different types of manipulations impact users' perceptions and how the context in which a modified image is used influences human perception of image deceptiveness.  Through an extensive crowdsourcing user study, we aim at demonstrating that the problem of predicting user-perceived deception can be approached by automatic methods. Analysis of results collected on Amazon Mechanical Turk platform highlights how deception is related to the level of modifications applied to the image and to the context within modified pictures are used. To the best of our knowledge, this work represents the first attempt to address to the image editing debate using automatic approaches and going beyond investigation of forgeries.},
	Author = {Conotter, Valentina and Dang-Nguyen, Duc-Tien and Boato, Giulia and Menendez, Maria and Larson, Martha},
	Booktitle = {Proc. SPIE 9014, Human Vision and Electronic Imaging XIX},
	Doi = {10.1117/12.2039418},
	Pages = {90140Y},
	Title = {Assessing the impact of image manipulation on users' perceptions of deception},
	Volume = {9014},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1117/12.2039418}}

@inproceedings{dangnguyen2013counterforensic,
	Abstract = {Median filtering is a well-known non linear denoising filter often used as an harmless post-processing, sometimes also employed to affect the reliability of some forensic techniques. In this work, we present a novel counter-forensic method able to conceal the characteristic traces left by median filtering. By exploiting the knowledge of features used in existing median filtering detectors, we are able to remove the characteristic footprints via suitable random pixel modification, while keeping the quality of the counter-attacked image high. Experimental results show that the proposed method is very effective, computationally efficient and competitive with other state-of-the-art techniques.},
	Author = {Dang-Nguyen, Duc-Tien and Gebru, Israel Dejene and Conotter, Valentina and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Workshop on Multimedia Signal Processing (MMSP)},
	Doi = {10.1109/MMSP.2013.6659298},
	Pages = {260--265},
	Title = {Counter-forensics of median filtering},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/MMSP.2013.6659298}}

@inproceedings{rota2013exploiting,
	Abstract = {In this paper we propose a new method to infer human social interactions using typical techniques adopted in literature for visual search and information retrieval. The main piece of information we use to discriminate among different types of interactions is provided by proxemics cues acquired by a tracker, and used to distinguish between intentional and casual interactions. The proxemics information has been acquired through the analysis of two different metrics: on the one hand we observe the current distance between subjects, and on the other hand we measure the O-space synergy between subjects. The obtained values are taken at every time step over a temporal sliding window, and processed in the Discrete Fourier Transform (DFT) domain. The features are eventually merged into an unique array, and clustered using the K-means algorithm. The clusters are reorganized using a second larger temporal window into a Bag Of Words framework, so as to build the feature vector that will feed the SVM classifier.},
	Author = {Rota, Paolo and Dang-Nguyen, Duc-Tien and Conci, Nicola and Sebe, Nicu},
	Booktitle = {Proc. SPIE 8667, Multimedia Content and Mobile Devices},
	Doi = {10.1117/12.2005307},
	Pages = {86670C},
	Title = {Exploiting visual search theory to infer social interactions},
	Volume = {8667},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1117/12.2005307}}

@inproceedings{dangnguyen2012identify,
	Abstract = {Significant improvements have been recently achieved in both quality and realism of computer generated characters, which are nowadays often very difficult to be distinguished from real ones. However, generating highly realistic facial expressions is still a challenging issue, since synthetic expressions usually follow a repetitive pattern, while in natural faces the same expression is usually produced in similar but not equal ways. In this paper, we propose a method to distinguish between computer generated and natural faces based on facial expressions analysis. In particular, small variations of the facial shape models corresponding to the same expression are used as evidence of synthetic characters.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Workshop on Information Forensics and Security (WIFS)},
	Doi = {10.1109/WIFS.2012.6412658},
	Note = {{BEST PAPER AWARD}},
	Pages = {252--257},
	Title = {Identify computer generated characters by analysing facial expressions variation},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/WIFS.2012.6412658}}

@inproceedings{muratov2012saliency,
	Abstract = {In this work we introduce a new application of saliency detection as a support for forensic analysis in digital images. Indeed, although there are some attempts to achieve both detection and localization of image tampering, the majority of forensics tools require hypothesis about the modified area. Since we claim that salient objects/subjects, conveying the semantic content of the image, are the regions whose integrity is more critical, we propose to combine salient object detection and forensic analysis on the corresponding output. To this aim we introduce here an improved version of a saliency map extractor based on segmentation and describe the application of a tampering detection method allowing digital composite detection. Experimental results are presented and discussed.},
	Author = {Muratov, Oleg and Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Symposium on Communications Control and Signal Processing (ISCCSP)},
	Doi = {10.1109/ISCCSP.2012.6217880},
	Pages = {1--5},
	Title = {Saliency detection as a support for image forensics},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISCCSP.2012.6217880}}

@inproceedings{dangnguyen2012discrimination,
	Abstract = {The recent development of information and communication technology has made computer software able to create highly realistic multimedia contents that can be, for human, impossible to distinguish from the natural ones. This fact leads to the need for tools and techniques that can reliably discriminate between natural and computer generated multimedia data in forensics applications. In this paper, we focus on the specific class of images containing faces, since we consider critical to be able to discriminate between photographic faces and the photorealistic ones. To this aim, we present a new geometric-based approach relying on face asymmetry information. Experimental results show that asymmetry information could be used as a hint to tackle this problem without requiring classification tools and training or combined with state-of-the-art approaches to improve their performances.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {European Signal Processing Conference (EUSIPCO)},
	Issn = {2219-5491},
	Keywords = {Digital Image Forensics, Computer Generated Multimedia Content},
	Pages = {1234--1238},
	Title = {Discrimination Between Computer Generated and Natural Human Faces Based on Asymmetry Information},
	Year = {2012}}

@inproceedings{dangnguyen2012supervisedmodels,
	Abstract = {Nowadays, large-scale networked social media need better search technologies to achieve suitable performance. Multimodal approaches are promising technologies to improve image ranking. This is particularly true when metadata are not completely reliable, which is a rather common case as far as user annotation, time and location are concerned. In this paper, we propose to properly combine visual information with additional multi-faceted information, to define a novel multimodal similarity measure. More specifically, we combine visual features, which strongly relate to the image content, with semantic information represented by manually annotated concepts, and geo tagging, very often available in the form of object/subject location. Furthermore, we propose a supervised machine learning approach, based on Support Vector Machines (SVMs), to automatically learn optimized weights to combine the above features. The resulting models is used as a ranking function to sort the results of a multimodal query.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and Moschitti, Alessandro and De Natale, Francesco G B},
	Booktitle = {IEEE International Workshop on Content-Based Multimedia Indexing (CBMI)},
	Doi = {10.1109/CBMI.2012.6269806},
	Issn = {1949-3983},
	Pages = {1--5},
	Title = {Supervised models for multimodal image retrieval based on visual, semantic and geographic information},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CBMI.2012.6269806}}

@inproceedings{dao2011signature,
	Abstract = {Quick reorganizing and draft annotating personal photo albums under event scheme is an emerging trend. In this research, a method has been developed to meet such requirements using the idea of gist and mosaic art so that viewers could understand the meaning of a whole scene without paying much attention in individual details. First, given a photo album, all chronologically ordered images are normalized to a smaller size, and then mosaicked side-by-side to create a signature image representing for that album. Next, by integrating the optimized linear programming with the color descriptor of the signature image, not only the event-type of the album but also all sub-event-types of the sub-sequence photos are decided. More than 19,000 images of five varied event-types have been used to evaluate the proposed method. Experimental results show that the proposed method could detect events towards annotation and re-organization of personal photo albums with high accuracy at a rapid speed.},
	Author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and De Natale, Francesco G B},
	Booktitle = {ACM International Conference on Multimedia},
	Doi = {10.1145/2072298.2072045},
	Isbn = {978-1-4503-0616-4},
	Keyword = {Gist, Mosaic art, Optimization Linear Programming, Event Analysis, Personal Photo Album},
	Pages = {1481--1484},
	Title = {Signature-image-based event analysis for personal photo albums},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2072298.2072045}}
